---
title: "Data preprocessing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/scratch/dongelr1/susannar/kesa2024")

library(dplyr)
library(purrr)
library('stringr')
library(lubridate)
library(zoo)
library(ggplot2)
library(ggpubr)
library(tidyr)
library("scales")
library(hdf5r)
library(timetk)
library(caret)
```

Preprocess and merge the SMEAR II data into one dataframe
```{r}
rename_smear_columns <- function(df) {
  names(df)[names(df) == "HYY_META.Pamb0"] <- "air_pressure"
  names(df)[names(df) == "HYY_META.T336"] <- "temperature"
  names(df)[names(df) == "HYY_META.Glob"] <- "global_radiation"
  names(df)[names(df) == "HYY_META.NOx336"] <- "NOx"
  names(df)[names(df) == "HYY_META.O3336"] <- "O3"
  names(df)[names(df) == "HYY_META.RHTd"] <- "relative_humidity"
  names(df)[names(df) == "HYY_META.SO2168"] <- "SO2"
  names(df)[names(df) == "HYY_META.WDU336"] <- "wind_direction"
  names(df)[names(df) == "HYY_META.WSU336"] <- "wind_speed"
  return(df)
}

merge_smear_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(read.csv) %>%
    lapply(function(x) {x$Time <- as.POSIXct(str_c(x$Year, "-", x$Month, "-", x$Day, " ", x$Hour, ":", x$Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC");x}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    lapply(function(x) x[!names(x) %in% c("Minute","Second", "Hour", "Day", "Year", "Month")]) %>%
    lapply(function(x) rename_smear_columns(x)) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(cbind) %>%
    .[unique(colnames(.))]
  return(x)
}

path1113 <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/smear_hyytiala_2011_2013/"
path1819 <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/smear_hyytiala_2018_2019/"
path22 <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/smear_hyytiala_2022/"
hyytiala1113 <- merge_smear_data(path1113)
hyytiala1819 <- merge_smear_data(path1819)
hyytiala22 <- merge_smear_data(path22)

smear_data <- reduce(list(hyytiala1113, hyytiala1819, hyytiala22), rbind)

write.csv(smear_data, "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/smear_merged.csv", row.names = FALSE)

# Split wind direction into two columns using sine/cosine transformation

smear_data$wdir_sin <- sin(pi * smear_data$wind_direction/180)
smear_data$wdir_cos <- cos(pi * smear_data$wind_direction/180)

# Replace temperature with kelvins, drop the wind_direction column

smear_data <- smear_data %>% mutate(temp_K = temperature + 273.15) %>% dplyr::select(-wind_direction, -temperature)

# Create the log transformed data
log_transform_data <- function(data) {
  data <- data %>% filter(global_radiation > 0) %>% filter(NOx > 0) %>% filter(SO2 > 0)
  log_features <- data %>% select(-Time, -wdir_sin, -wdir_cos) %>% colnames
  data <- data %>% mutate_at(log_features, log)
  return(data)
}

smear_data_transformed <- log_transform_data(smear_data)

# Set negative values in global_radiation, NOx and SO2 to 0
smear_data$global_radiation[smear_data$global_radiation < 0] <- 0
smear_data$NOx[smear_data$NOx < 0] <- 0
smear_data$SO2[smear_data$SO2 < 0] <- 0
```


Load and preprocess ToL data

```{r, warning=FALSE}

load_tol_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(function(x) read.csv(x, header = TRUE, col.names = c("Time", "ToL", "sector"))) %>%
    lapply(function(x) {y <- mutate(x, Time = as.POSIXct(Time, tz = "UTC", format = "%Y-%m-%d %H:%M:%S")); y}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(rbind)
  return(x)
}

# Function to impute the 30 min values
impute_missing_tol_data <- function(df) {
  tol_na_rem <- df %>% drop_na()
  
  sectors <- tol_na_rem %>% group_by(sector) %>% group_data() %>% split(.$sector)
  s1 <- sectors[[1]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s2 <- sectors[[2]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s3 <- sectors[[3]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s4 <- sectors[[4]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  
  pad_30min <- function(x, df) {
    df <- pad_by_time(df[x,], Time, .by = "30 mins", .pad_value = NA)
    return(df)
  }
  
  # Pad each of the sections separately
  padded_s1 <- lapply(s1, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind) 
  padded_s2 <- lapply(s2, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  padded_s3 <- lapply(s3, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  padded_s4 <- lapply(s4, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  
  # Linearly impute the ToL values on padded rows if the sector doesn't changes around them
  imputed_s1 <- lapply(padded_s1[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s2 <- lapply(padded_s2[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s3 <- lapply(padded_s3[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s4 <- lapply(padded_s4[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  
  padded_s1[2:3] = imputed_s1
  padded_s2[2:3] = imputed_s2
  padded_s3[2:3] = imputed_s3
  padded_s4[2:3] = imputed_s4
  
  tol_imputed <- rbind(padded_s1, padded_s2, padded_s3, padded_s4)
  tol_imputed <- tol_imputed %>% arrange(Time) %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)

  # Remove excess years generated
  tol_imputed <- filter(tol_imputed, year(Time) %in% c(2011, 2012, 2013, 2018, 2019, 2022))
  
  return(tol_imputed)
}

tol <- load_tol_data("/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/time_over_land/")
tol_imputed <- impute_missing_tol_data(tol)

# Add column for the name of the sector
sector_names <- c("clean", "europe", "east", "mixed")
names(tol_imputed)[names(tol_imputed) == "sector"] <- "sector_num"
tol_imputed <- tol_imputed %>%
  mutate(sector = sector_names[sector_num])

# Categorize the sector names
dmy <- dummyVars(" ~ sector", data = tol_imputed)
cat_df <- data.frame(predict(dmy, newdata = tol_imputed))
names(cat_df)[names(cat_df) == "sectorclean"] <- "sector.clean"
names(cat_df)[names(cat_df) == "sectoreurope"] <- "sector.europe"
names(cat_df)[names(cat_df) == "sectoreast"] <- "sector.east"
names(cat_df)[names(cat_df) == "sectormixed"] <- "sector.mixed"

tol_imputed <- cbind(tol_imputed, cat_df) %>% select(-sector_num, -sector)

tol_transformed <- tol_imputed %>% mutate_at(c("ToL"), log)

```


Load and preprocess the SA measurement data
```{r}
preprocess_sa_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(read.table) %>%
    lapply(function(x) {names(x) <- c("Time", "SA_cm3", "3", "4"); x}) %>%
    lapply(function(x) {x[3:4] <- list(NULL); x}) %>%
    lapply(function(x) {y <- mutate(x, Time=as.POSIXct((Time - 719529)*86400, origin = "1970-01-01", tz="UTC")); y}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(rbind) %>%
    arrange(Time) %>%
    filter((SA_cm3 > -1e5) & (SA_cm3 < 1e9))
    return(x)
}

sa_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/sulphuric_acid/"
sa <- preprocess_sa_data(sa_path)

sa11 <- filter(sa, year(Time) == 2011)
sa12 <- filter(sa, year(Time) == 2012)
sa13 <- filter(sa, year(Time) == 2013)
sa18 <- filter(sa, year(Time) == 2018)
sa19 <- filter(sa, year(Time) == 2019)
sa22 <- filter(sa, year(Time) == 2022)

# SA data in 2011 and 2012 is measured every 2 minutes (other data is every 30 min). Before calculating the 30 min average, filter out possible outliers using interquartile range (IQR): filter out data points that are further than 1.5 * IQR away from Q1 or Q3 of the data in the current window. If the window doesn't fit (in the beginning/end), just keep the data as is.
iqr_filter <- function(d, width, q1 = 0.25, q2 = 0.75) {
  filtered <- d %>%
    mutate(lower = rollapply(
      d$SA_cm3, width, align = "center",  fill = NA,
      FUN = function(x) {
        r <- quantile(x, probs = c(0.25, 0.75));
        iqr <- r[2] - r[1];
        lower_lim <- r[1] - 1.5*iqr;
        upper_lim <- r[2] + 1.5*iqr;
        lower_lim
      })) %>%
    mutate(upper = rollapply(
      d$SA_cm3, width, align = "center",  fill = NA,
      FUN = function(x) { 
        r <- quantile(x, probs = c(0.25, 0.75));
        iqr <- r[2] - r[1];
        lower_lim <- r[1] - 1.5*iqr;
        upper_lim <- r[2] + 1.5*iqr;
        upper_lim})) %>%
    filter(is.na(lower) | between(SA_cm3, lower, upper)) %>%
    mutate(lower = NULL, upper = NULL)
  return(filtered)
}

sa11_filtered_30 <- iqr_filter(sa11, width = 15)
sa12_filtered_30 <- iqr_filter(sa12, width = 15)

removed_f30 <- setdiff(sa11, sa11_filtered_30)
saveRDS(removed_f30, "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/left_out_sa_data_f30.rds")

# sa11_filtered_60 <- iqr_filter(sa11, width = 31)
# sa12_filtered_60 <- iqr_filter(sa12, width = 31)
# 
# sa11_filtered_240 <- iqr_filter(sa11, width = 121)
# sa12_filtered_240 <- iqr_filter(sa12, width = 121)


# Round the SA values to 30 min
compute_SA_30min_average <- function(df) {
  df$Time <- lubridate::floor_date(df$Time, "30 minutes")
  df <- group_by(df, Time) %>% summarize(SA_cm3 = mean(SA_cm3, na.rm = FALSE))
  return(df)
}

sa11_rounded <- compute_SA_30min_average(sa11)
sa12_rounded <- compute_SA_30min_average(sa12)

sa11_rounded_f30 <- compute_SA_30min_average(sa11_filtered_30)
sa12_rounded_f30 <- compute_SA_30min_average(sa12_filtered_30)

# sa11_rounded_f60 <- compute_SA_30min_average(sa11_filtered_60)
# sa12_rounded_f60 <- compute_SA_30min_average(sa12_filtered_60)
# 
# sa11_rounded_f240 <- compute_SA_30min_average(sa11_filtered_240)
# sa12_rounded_f240 <- compute_SA_30min_average(sa12_filtered_240)


# 2013 seems to contain duplicate rows. Keep only unique rows and round the dates to nearest 30 min to round times such as 11:29:59
sa13 <- sa13 %>% distinct(.keep_all = TRUE)
sa13$Time <- round_date(sa13$Time, unit = "30 mins")


# Round dates in 2018, 2019 and 2022 to the nearest 30 mins to round times such as 11:29:59
sa18$Time <- round_date(sa18$Time, unit = "30 mins")
sa19$Time <- round_date(sa19$Time, unit = "30 mins")
sa22$Time <- round_date(sa22$Time, unit = "30 mins")

sa_data <- reduce(list(sa11_rounded, sa12_rounded, sa13, sa18, sa19, sa22), rbind)
sa_data_30 <- reduce(list(sa11_rounded_f30, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
# sa_data_60 <- reduce(list(sa11_rounded_f60, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
# sa_data_240 <- reduce(list(sa11_rounded_f240, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
```


Alternative filtering to compare with the IQR filtering above

```{r}
library(pracma)

test <- setdiff(sa11, sa11_filtered_30)
fout_sa11 <- test %>% mutate(date = as.Date(Time))
f_sa11 <- sa11_filtered_30 %>% mutate(date = as.Date(Time))
remp <- dim(fout_sa11)[[1]]/dim(sa11)[[1]]

dates <- unique(fout_sa11$date)

plot_day <- function(date, df, df_out) {
  dat <- filter(df, date == !!date)
  dat_out <- filter(df_out, date == !!date)

  p <- ggplot() +
    geom_point(data = dat_out, aes(x = Time, y = SA_cm3), color = "red") +
    geom_line(data = dat, aes(x = Time, y = SA_cm3)) +
    xlab("Time") +
    ylab("SA") +
    # scale_y_continuous(trans = "log10") +
    ggtitle(date)
}

plots <- lapply(dates, plot_day, df = f_sa11, df_out = fout_sa11)
g <- ggarrange(plotlist = plots, common.legend = TRUE, legend = "bottom")
g <- annotate_figure(g, top = text_grob(paste("Current IQR method", remp)))

window_size = 7 # total 2 * window_size + 1
sa11_wdate <- mutate(sa11, date = as.Date(Time))
res1 <- hampel(x = sa11_wdate$SA_cm3, k = window_size, t0 = 3) # Pearson's 3 sigma edit rule
res2 <- hampel(x = sa11_wdate$SA_cm3, k = window_size, t0 = 0) # John Tukey's median filter
res3 <- hampel(x = sa11_wdate$SA_cm3, k = window_size, t0 = 1) # ???

f1 <- sa11_wdate[setdiff(1:dim(sa11_wdate)[[1]], res1$ind),]
f1_out <- sa11_wdate[res1$ind,]
remp1 <- length(res1$ind)/dim(sa11_wdate)[[1]]

dates <- unique(f1_out$date)
plots1 <- lapply(dates, plot_day, df = f1, df_out = f1_out)
g1 <- ggarrange(plotlist = plots1, common.legend = TRUE, legend = "bottom")
g1 <- annotate_figure(g1, top = text_grob(paste("Hampel with Pearson's sigma", remp1)))

f2 <- sa11_wdate[setdiff(1:dim(sa11_wdate)[[1]], res2$ind),]
f2_out <- sa11_wdate[res2$ind,]
remp2 <- length(res2$ind)/dim(sa11_wdate)[[1]]

dates <- unique(f2_out$date)
plots2 <- lapply(dates, plot_day, df = f2, df_out = f2_out)
g2 <- ggarrange(plotlist = plots2, common.legend = TRUE, legend = "bottom")
g2 <- annotate_figure(g2, top = text_grob(paste("Hampel with Tukey's median", remp2)))

f3 <- sa11_wdate[setdiff(1:dim(sa11_wdate)[[1]], res3$ind),]
f3_out <- sa11_wdate[res3$ind,]
remp3 <- length(res3$ind)/dim(sa11_wdate)[[1]]

dates <- unique(f3_out$date)
plots3 <- lapply(dates, plot_day, df = f3, df_out = f3_out)
g3 <- ggarrange(plotlist = plots3, common.legend = TRUE, legend = "bottom")
g3 <- annotate_figure(g3, top = text_grob(paste("Something in between?", remp3)))

f1_avg <-  compute_SA_30min_average(f1)
f2_avg <-  compute_SA_30min_average(f2)
f3_avg <-  compute_SA_30min_average(f3)
```


Load and preprocess the condensation sink data
```{r}
cs_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/condensation_sink/CS_2004_2023_10min.txt"
cs_all <- read.table(cs_path, col.names = c("Year", "Month", "Day", "Hour", "Minute", "Second", "CS_rate"))
cs <- filter(cs_all, (Year >= 2011 & Year <= 2013) | (Year >= 2018 & Year <= 2019) | (Year == 2022))
cs$Time <- as.POSIXct(str_c(cs$Year, "-", cs$Month, "-", cs$Day, " ", cs$Hour, ":", cs$Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC")
cs <- cs[!names(cs) %in% c("Minute","Second", "Hour", "Day", "Year", "Month")]

cs$Time <- floor_date(cs$Time, "30 mins")
cs <- group_by(cs, Time)
cs <- summarise(cs, CS_rate = mean(CS_rate, na.rm = FALSE))

cs_transformed <- cs %>% mutate_at(c("CS_rate"), log)
```

```{r}
preprocess_cs_data <- function() {
  cs_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/raw/condensation_sink/CS_2004_2023_10min.txt"
  cs <- read.table(cs_path, col.names = c("Year", "Month", "Day", "Hour", "Minute", "Second", "CS_rate")) %>%
    filter((Year >= 2011 & Year <= 2013) | (Year >= 2018 & Year <= 2019) | (Year == 2022)) %>%
    mutate(Time = as.POSIXct(str_c(Year, "-", Month, "-", Day, " ", Hour, ":", Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC")) %>%
    dplyr::select(-Minute, -Second, -Hour, -Day, -Month, -Year)
    
    cs$Time <- floor_date(cs$Time, "30 mins")
    cs <- group_by(cs, Time) %>%
      summarise(CS_rate = mean(CS_rate, na.rm = FALSE))
    return(cs)
}

cs2 <- preprocess_cs_data()
```



Combine SA, CS and the SMEAR data into one data frame and write to file.
```{r}
all_data <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data, cs, tol_imputed))
all_data1 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_30, cs, tol_imputed))
# all_data2 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_60, cs, tol_imputed))
# all_data3 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_240, cs, tol_imputed))
all_data_transformed <- Reduce(function(x, y) merge (x, y, all=TRUE), list(smear_data_transformed, sa_data, cs_transformed, tol_transformed))
all_data_transformed_filtered <- Reduce(function(x, y) merge (x, y, all=TRUE), list(smear_data_transformed, sa_data_30, cs_transformed, tol_transformed))


write.csv(all_data, "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/preprocessed/unfiltered_sa.csv", row.names = FALSE)
write.csv(all_data1, "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/preprocessed/f30.csv", row.names = FALSE)
# write.csv(all_data2, "data/all_data_merged_f60.csv", row.names = FALSE)
# write.csv(all_data3, "data/all_data_merged_f240.csv", row.names = FALSE)
write.csv(all_data_transformed, "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/preprocessed/logtransformed.csv", row.names = FALSE)
write.csv(all_data_transformed_filtered, "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/preprocessed/logtransformed_f30.csv", row.names = FALSE)
```
