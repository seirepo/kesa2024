---
title: "Data preprocessing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/scratch/dongelr1/susannar/kesa2024")

library(dplyr)
library(purrr)
library('stringr')
library(lubridate)
library(zoo)
library(ggplot2)
library(ggpubr)
library(tidyr)
library("scales")
library(hdf5r)
library(timetk)
```

Preprocess and merge the SMEAR II data into one dataframe
```{r}
rename_smear_columns <- function(df) {
  names(df)[names(df) == "HYY_META.Pamb0"] <- "air_pressure"
  names(df)[names(df) == "HYY_META.T336"] <- "temperature"
  names(df)[names(df) == "HYY_META.Glob"] <- "global_radiation"
  names(df)[names(df) == "HYY_META.NOx336"] <- "NOx"
  names(df)[names(df) == "HYY_META.O3336"] <- "O3"
  names(df)[names(df) == "HYY_META.RHTd"] <- "relative_humidity"
  names(df)[names(df) == "HYY_META.SO2168"] <- "SO2"
  names(df)[names(df) == "HYY_META.WDU336"] <- "wind_direction"
  names(df)[names(df) == "HYY_META.WSU336"] <- "wind_speed"
  return(df)
}

merge_smear_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(read.csv) %>%
    lapply(function(x) {x$Time <- as.POSIXct(str_c(x$Year, "-", x$Month, "-", x$Day, " ", x$Hour, ":", x$Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC");x}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    lapply(function(x) x[!names(x) %in% c("Minute","Second", "Hour", "Day", "Year", "Month")]) %>%
    lapply(function(x) rename_smear_columns(x)) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(cbind) %>%
    .[unique(colnames(.))]
  return(x)
}

path1113 <- "/scratch/dongelr1/susannar/kesa2024/data/smear_hyytiala_2011_2013/"
path1819 <- "/scratch/dongelr1/susannar/kesa2024/data/smear_hyytiala_2018_2019/"
path22 <- "/scratch/dongelr1/susannar/kesa2024/data/smear_hyytiala_2022/"
hyytiala1113 <- merge_smear_data(path1113)
hyytiala1819 <- merge_smear_data(path1819)
hyytiala22 <- merge_smear_data(path22)

smear_data <- reduce(list(hyytiala1113, hyytiala1819, hyytiala22), rbind)

# Set negative values in global_radiation, NOx and SO2 to 0
smear_data$global_radiation[smear_data$global_radiation < 0] <- 0
smear_data$NOx[smear_data$NOx < 0] <- 0
smear_data$SO2[smear_data$SO2 < 0] <- 0

# Split wind direction into two columns using sine/cosine transformation

smear_data$wdir_sin <- sin(pi * smear_data$wind_direction/180)
smear_data$wdir_cos <- cos(pi * smear_data$wind_direction/180)

```


Load and preprocess ToL data

```{r, warning=FALSE}

load_tol_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(function(x) read.csv(x, header = TRUE, col.names = c("Time", "ToL", "sector"))) %>%
    lapply(function (x) {y <- mutate(x, Time = as.POSIXct(Time, tz = "UTC", format = "%Y-%m-%d %H:%M:%S")); y}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(rbind)
  return(x)
}

# Function to impute the 30 min values
impute_missing_tol_data <- function(df) {
  tol_na_rem <- df %>% drop_na()
  
  sectors <- tol_na_rem %>% group_by(sector) %>% group_data() %>% split(.$sector)
  s1 <- sectors[[1]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s2 <- sectors[[2]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s3 <- sectors[[3]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s4 <- sectors[[4]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  
  pad_30min <- function(x, df) {
    df <- pad_by_time(df[x,], Time, .by = "30 mins", .pad_value = NA)
    return(df)
  }
  
  # Pad each of the sections separately
  padded_s1 <- lapply(s1, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind) 
  padded_s2 <- lapply(s2, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  padded_s3 <- lapply(s3, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  padded_s4 <- lapply(s4, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  
  imputed_s1 <- lapply(padded_s1[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s2 <- lapply(padded_s2[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s3 <- lapply(padded_s3[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s4 <- lapply(padded_s4[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  
  padded_s1[2:3] = imputed_s1
  padded_s2[2:3] = imputed_s2
  padded_s3[2:3] = imputed_s3
  padded_s4[2:3] = imputed_s4
  
  tol_imputed <- rbind(padded_s1, padded_s2, padded_s3, padded_s4)
  tol_imputed <- tol_imputed %>% arrange(Time) %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)

  # Remove excess years generated
  tol_imputed <- filter(tol_imputed, year(Time) %in% c(2011, 2012, 2013, 2018, 2019, 2022))
  
  return(tol_imputed)
}


tol <- load_tol_data("/scratch/dongelr1/susannar/kesa2024/data/hyytiala/time_over_land/")
tol_imputed <- impute_missing_tol_data(tol)

```


Load and preprocess the SA measurement data
```{r}
preprocess_sa_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(read.table) %>%
    lapply(function(x) {names(x) <- c("Time", "SA_cm3", "3", "4"); x}) %>%
    lapply(function(x) {x[3:4] <- list(NULL); x}) %>%
    lapply(function(x) {y <- mutate(x, Time=as.POSIXct((Time - 719529)*86400, origin = "1970-01-01", tz="UTC")); y}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(rbind) %>%
    arrange(Time) %>%
    filter((SA_cm3 > -1e5) & (SA_cm3 < 1e9))
    return(x)
}

sa_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/sulphuric_acid/"
sa <- preprocess_sa_data(sa_path)

sa11 <- filter(sa, year(Time) == 2011)
sa12 <- filter(sa, year(Time) == 2012)
sa13 <- filter(sa, year(Time) == 2013)
sa18 <- filter(sa, year(Time) == 2018)
sa19 <- filter(sa, year(Time) == 2019)
sa22 <- filter(sa, year(Time) == 2022)

# SA data in 2011 and 2012 is measured every 2 minutes (other data is every 30 min). Before calculating the 30 min average, filter out possible outliers outliers using interquartile range (IQR): filter out data points that are further than 1.5 * IQR away from Q1 or Q3 of the data in the current window. If the window doesn't fit (in the beginning/end), just keep the data as is.

iqr_filter <- function(d, width, q1 = 0.25, q2 = 0.75) {
  filtered <- d %>%
    mutate(lower = rollapply(d$SA_cm3, width, FUN = function(x) { r <- quantile(x, probs = c(0.25, 0.75)); iqr <- r[2] - r[1]; lower_lim <- r[1] - 1.5*iqr; upper_lim <- r[2] + 1.5*iqr; lower_lim}, align = "center",  fill = NA)) %>%
    mutate(upper = rollapply(d$SA_cm3, width, FUN = function(x) { r <- quantile(x, probs = c(0.25, 0.75)); iqr <- r[2] - r[1]; lower_lim <- r[1] - 1.5*iqr; upper_lim <- r[2] + 1.5*iqr; upper_lim}, align = "center",  fill = NA)) %>%
    filter(is.na(lower) | between(SA_cm3, lower, upper)) %>%
    mutate(lower = NULL, upper = NULL)
  return(filtered)
}

sa11_filtered_30 <- iqr_filter(sa11, width = 15)
sa12_filtered_30 <- iqr_filter(sa12, width = 15)

sa11_filtered_60 <- iqr_filter(sa11, width = 31)
sa12_filtered_60 <- iqr_filter(sa12, width = 31)

sa11_filtered_240 <- iqr_filter(sa11, width = 121)
sa12_filtered_240 <- iqr_filter(sa12, width = 121)


# Round the SA values to 30 min
compute_SA_30min_average <- function(x) {
  df <- data.frame(x)
  df$Time <- floor_date(df$Time, "30 mins")
  df <- group_by(df, Time)
  df <- summarise(df, SA_cm3 = mean(SA_cm3, na.rm = FALSE))
  return(df)
}

sa11_rounded <- compute_SA_30min_average(sa11)
sa12_rounded <- compute_SA_30min_average(sa12)

sa11_rounded_f30 <- compute_SA_30min_average(sa11_filtered_30)
sa12_rounded_f30 <- compute_SA_30min_average(sa12_filtered_30)

sa11_rounded_f60 <- compute_SA_30min_average(sa11_filtered_60)
sa12_rounded_f60 <- compute_SA_30min_average(sa12_filtered_60)

sa11_rounded_f240 <- compute_SA_30min_average(sa11_filtered_240)
sa12_rounded_f240 <- compute_SA_30min_average(sa12_filtered_240)


# 2013 seems to contain duplicate rows. Keep only unique rows and round the dates to nearest 30 min to round times such as 11:29:59
sa13 <- sa13 %>% distinct(.keep_all = TRUE)
sa13$Time <- round_date(sa13$Time, unit = "30 mins")


# Round dates in 2018, 2019 and 2022 to the nearest 30 mins to round times such as 11:29:59
sa18$Time <- round_date(sa18$Time, unit = "30 mins")
sa19$Time <- round_date(sa19$Time, unit = "30 mins")
sa22$Time <- round_date(sa22$Time, unit = "30 mins")

sa_data <- reduce(list(sa11_rounded, sa12_rounded, sa13, sa18, sa19, sa22), rbind)
sa_data_30 <- reduce(list(sa11_rounded_f30, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
sa_data_60 <- reduce(list(sa11_rounded_f60, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
sa_data_240 <- reduce(list(sa11_rounded_f240, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
```


Load and preprocess the condensation sink data
```{r}
cs_path <- "data/hyytiala/condensation_sink/CS_2004_2023_10min.txt"
cs_all <- read.table(cs_path, col.names = c("Year", "Month", "Day", "Hour", "Minute", "Second", "CS_rate"))
cs <- filter(cs_all, (Year >= 2011 & Year <= 2013) | (Year >= 2018 & Year <= 2019) | (Year == 2022))
cs$Time <- as.POSIXct(str_c(cs$Year, "-", cs$Month, "-", cs$Day, " ", cs$Hour, ":", cs$Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC")
cs <- cs[!names(cs) %in% c("Minute","Second", "Hour", "Day", "Year", "Month")]

cs$Time <- floor_date(cs$Time, "30 mins")
cs <- group_by(cs, Time)
cs <- summarise(cs, CS_rate = mean(CS_rate, na.rm = FALSE))
```


Combine SA, CS and the SMEAR data into one data frame and write to file.
```{r}
all_data <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data, cs, tol_imputed))
all_data1 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_30, cs, tol_imputed))
all_data2 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_60, cs, tol_imputed))
all_data3 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_240, cs, tol_imputed))

write.csv(all_data, "data/all_data_merged.csv", row.names = FALSE)
write.csv(all_data1, "data/all_data_merged_f30.csv", row.names = FALSE)
write.csv(all_data2, "data/all_data_merged_f60.csv", row.names = FALSE)
write.csv(all_data3, "data/all_data_merged_f240.csv", row.names = FALSE)
```
