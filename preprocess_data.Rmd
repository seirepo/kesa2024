---
title: "Data preprocessing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("/scratch/dongelr1/susannar/kesa2024")

library(dplyr)
library(purrr)
library('stringr')
library(lubridate)
library(zoo) # rollapply
library(ggplot2)
library(ggpubr)
library(tidyr) # pivot_longer
library("scales")
library(hdf5r)
library(timetk)
```

Preprocess and merge the SMEAR II data into one dataframe
```{r}
rename_smear_columns <- function(df) {
  names(df)[names(df) == "HYY_META.Pamb0"] <- "air_pressure"
  names(df)[names(df) == "HYY_META.T336"] <- "temperature"
  names(df)[names(df) == "HYY_META.Glob"] <- "global_radiation"
  names(df)[names(df) == "HYY_META.NOx336"] <- "NOx"
  names(df)[names(df) == "HYY_META.O3336"] <- "O3"
  names(df)[names(df) == "HYY_META.RHTd"] <- "relative_humidity"
  names(df)[names(df) == "HYY_META.SO2168"] <- "SO2"
  names(df)[names(df) == "HYY_META.WDU336"] <- "wind_direction"
  names(df)[names(df) == "HYY_META.WSU336"] <- "wind_speed"
  return(df)
}

merge_smear_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(read.csv) %>%
    lapply(function(x) {x$Time <- as.POSIXct(str_c(x$Year, "-", x$Month, "-", x$Day, " ", x$Hour, ":", x$Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC");x}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    lapply(function(x) x[!names(x) %in% c("Minute","Second", "Hour", "Day", "Year", "Month")]) %>%
    lapply(function(x) rename_smear_columns(x)) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(cbind) %>%
    .[unique(colnames(.))]
  return(x)
}

path1113 <- "/scratch/dongelr1/susannar/kesa2024/data/smear_hyytiala_2011_2013/"
path1819 <- "/scratch/dongelr1/susannar/kesa2024/data/smear_hyytiala_2018_2019/"
path22 <- "/scratch/dongelr1/susannar/kesa2024/data/smear_hyytiala_2022/"
hyytiala1113 <- merge_smear_data(path1113)
hyytiala1819 <- merge_smear_data(path1819)
hyytiala22 <- merge_smear_data(path22)

smear_data <- reduce(list(hyytiala1113, hyytiala1819, hyytiala22), rbind)

# Set negative values in global_radiation, NOx and SO2 to 0
smear_data$global_radiation[smear_data$global_radiation < 0] <- 0
smear_data$NOx[smear_data$NOx < 0] <- 0
smear_data$SO2[smear_data$SO2 < 0] <- 0

# Split wind direction into two columns using sine/cosine transformation

smear_data$wdir_sin <- sin(pi * smear_data$wind_direction/180)
smear_data$wdir_cos <- cos(pi * smear_data$wind_direction/180)

```

plot wdir
```{r}
t <- filter(smear_data, as.Date(Time) == as.Date("2011-01-01"))
ggplot(t, aes(x=wdir_sin, y=wdir_cos), color = as.Date(Time)) +
    geom_point(shape=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  scale_y_continuous(limits = c(-1, 1))
```

Load and preprocess ToL data
```{r}
load_tol_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(function(x) read.csv(x, header = TRUE, col.names = c("Time", "ToL", "sector"))) %>%
    #lapply(function(x) {print(nrow(x)); x}) %>%
    lapply(function (x) {y <- mutate(x, Time = as.POSIXct(Time, tz = "UTC", format = "%Y-%m-%d %H:%M:%S")); y}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(rbind)
  return(x)
}

tol <- load_tol_data("/scratch/dongelr1/susannar/kesa2024/data/hyytiala/time_over_land/")

#test <- read.table("tol_2011_test.csv", sep = ",", header = FALSE, col.names = c("Time", "tol", "sector"))
test <- read.table("data/tol_2018_test.csv", sep = ",", header = TRUE, col.names = c("Time", "tol", "sector"))
#test <- mutate(test, Time=as.POSIXct((Time - 719529)*86400, origin = "1970-01-01", tz="UTC", format="%Y-%m-%d %H:%M:%S"))
test <- mutate(test, Time=as.POSIXct(Time, tz = "UTC", format = "%Y-%m-%d %H:%M:%S"))
#View(test)

summary(tol)

tol <- tol %>% mutate(DayMonth = format(Time, "%d-%m")) %>% mutate(DayMonth = paste(DayMonth, "2000", sep="-")) %>% mutate(DayMonth = as.Date(DayMonth, format = "%d-%m-%Y"))

#tol %>% mutate(DayMonth = format(Time, "%d-%m")) %>% mutate(DayMonth = paste(DayMonth, "2000", sep="-")) %>% mutate(DayMonth = as.Date(DayMonth, format = "%d-%m-%Y")) %>%  ggplot(aes(x = DayMonth, y = ToL, group = sector)) + geom_point(na.rm = TRUE, aes(shape = factor(sector), color = factor(sector))) + facet_wrap(~year(Time)) + scale_x_date(breaks = "1 month", labels = date_format("%m"))

p <- ggplot(tol, aes(x = DayMonth, y = ToL, group = sector)) + geom_point(na.rm = TRUE, aes(shape = factor(sector), color = factor(sector))) + facet_wrap(~year(Time)) + scale_x_date(breaks = "3 months", labels = date_format("%m"))
p

```

Plot the distribution of ToL for each year, grouped by sector
```{r}
ggplot(tol, aes(x = DayMonth), group = sector) + geom_histogram(na.rm = TRUE, aes(shape = factor(sector), color = factor(sector))) + facet_wrap(~year(Time)) + scale_x_date(breaks = "1 month", labels = date_format("%m")) + theme(axis.text.x = element_text(angle = 45, hjust = 1.5))


# Split tol by year, resulting in a list with the years as names and data frames as "values"
tol_by_year <- split(tol, year(tol$Time))

#tol11 <- filter(tol, year(Time) == 2011)
#tol12 <- filter(tol, year(Time) == 2012)
#tol13 

for (year in names(tol_by_year)) {
  tol_year <- tol_by_year[[year]]
  
  p <- ggplot(tol_year, aes(x = ToL)) + #, fill = sector)) +
    geom_histogram(binwidth = 5, aes(colour = factor(sector)), na.rm = TRUE) + #position = "dodge", na.rm = TRUE) +
    labs(title = paste("ToL", year, "(1=Clean, 2=Europe, 3=East, 4=Mixed)"),
         x = "ToL",
         y = "Count") +
    theme_minimal() +
    theme(legend.position = "bottom") +
    facet_wrap(~sector)
  
  print(p)
}


```


## TODO:

- [x] Check how many NAs there are, are there weird looking ToL values etc.
- [x] There's an NA sector during years 2018 and 2018 with no observations, remove such rows? No?
- [ ] Sector 4 is mixed ("ie. none"). Do we keep it? Yes?

```{r}
t18 <- tol_by_year[["2018"]]
t19 <- tol_by_year[["2019"]]

# It appears that the NA sectors are related to NA ToLs and vice versa which makes sense, if the trajectory couldn't be tracked, then both ToL and sector are missing
#View(filter(t19, is.na(sector)))
#View(filter(t19, is.na(ToL)))
#View(filter(tol_by_year[["2011"]], is.na(ToL)))


na2018 <- filter(tol_by_year[["2018"]], is.na(ToL))
ggplot(na2018, aes(x=ToL)) + geom_histogram(na.rm=TRUE, stat = "count") + theme(axis.text.x = element_text(angle = 45, hjust = 1))


# NA values only in 2018 and 2019, there aren't many of them
for (year in names(tol_by_year)) {
  tol_year <- tol_by_year[[year]]
  na_year <- filter(tol_year, is.na(ToL))
  p <- ggplot(na_year, aes(x=Time), group = sector) + geom_histogram(na.rm=TRUE) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle(paste("NAs,", year)) + scale_x_datetime(breaks = "1 month", labels = date_format("%d-%m"))
  print(p)
}

sapply(t18[ , 2:3], function(x) sum(is.na(x)))
sapply(t19[ , 2:3], function(x) sum(is.na(x)))


```


### Impute 30 min values

#### Approach 1
Iterate the ToL data frame row by row: split it when the sector changes and collect the splitted data frames into a list; apply `approxfun` or similar to linearly impute missing half hour values in each of the splitted dataframes; `rbind` the splitted data frames into one data frame; impute NA as the missing half hour values (expectation: NAs are inserted between the hours when the sector changes)
- Think of what to do for values that are already NA in the splitted data frames? Let them stay NA?

```{r}
df <- data.frame(
  Time = seq(as.POSIXct("2011-01-01 00:00:00", tz = "UTC"), as.POSIXct("2011-01-01 10:00:00", tz = "UTC"), by = "hour"),
  ToL =  floor(rnorm(n = 11, mean = 50, sd = 10)),
  sector = c(1, 2, 2, 3, 3, 4, 4, 4, 3, 3, 1)
  #sector = c(1, 2, 1, 2, 2, 2, 3, 3, 1, 4, 4, 1, 2, 3, 2, 4, 2, 4, 2, 2, 4, 2, 1, 3)
)


split_consecutive <- function(indices) {
  split_list <- split(indices, cumsum(c(TRUE, diff(indices) != 1)))
  print(split_list)
  return(split_list)
}
test <- df %>% group_by(sector) %>% group_data() %>% mutate(consec_idx = map(.rows, split_consecutive))
#%>% mutate(function(x) { print(x$.rows)}) #map(function(x) { split(x$.rows, cumsum(c(1, diff(x) != 1))) })
idx <- test$consec_idx


# Extract the consecutive indices from the test dataframe to create a list of lists (= index lists)
# Iterate the list and for each index, pick corresponding rows from df and impute missing 30 min values linearly there
# Check the df: how many missing 30 min values there are?


#test <- test %>% mutate(consec_idx = map(.rows, split_consecutive))
#test$consec_idx
#View(test)

#v <- c(4, 5, 8, 9)
#res <- split(v, cumsum(c(1, diff(v) != 1)))

```


#### Approach 2:
1. First impute the missing half an hour values first with -1/NA tms as ToL and sector
2. Get consecutive -1/NA sequences (all or only of length 1?) and check what the sector is before and after the NA sequence
  - Jos on pidempi NA sequence, voi olla et se sector vaihtuu sen sisällä ja sillon ei voi imputoida?
3. If the sector stays the same, impute missing ToL value linearly and set the sector. If not, do nothing

```{r}
test_df <- filter(t18, DayMonth == as.Date("2000-05-10"))
test_df <- test_df %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)


#test_df %>% group_by(sector) %>% group_rows()
idx <- which(is.na(test_df$ToL))
#View(test_df[idx, ])

surrounding_idx <- function(x, df) {
  res <- c(x-1, x, x+1)
  return(res[res >= 1 & res <= nrow(df)])
}

impute_missing_ToL <- function(idx, df) {
  l <- idx[[1]]
  m <- idx[[2]]
  u <- idx[[3]]
  
  s1 <- df$sector[l]
  s2 <- df$sector[u]
  #print(paste(l, m, u, ", sectors ", s1, s2))
  
  # Check if both sectors exist and are the same before imputing the missing 30 min values
  if (!(is.na(s1) | is.na(s2)) & (s1 == s2)) {
    if (!(is.na(df$ToL[l]) | is.na(df$ToL[u]))) {
      res <- approxfun(1:3, df$ToL[l:u])(1:3)
      df[m, "ToL"] <- res[[2]]
      df[m, "sector"] <- s1
      #print(paste("inserted", res[[2]], "at index", m, ":", df$ToL[m]))
    } else {
      #print(paste("l or u is na ", df$ToL[l], df$ToL[m], df$ToL[u]))
    }
  } else {
      #print("sector change or sector NA")
  }
  # Return the splitted data frame, modified or not
  return(df[l:u, ])
}

idx_extended <- lapply(idx, surrounding_idx, df = test_df)
idx_extended <- idx_extended[length(idx_extended) != 3]

#lapply(idx_extended[1:3], function(x) {d <- test_df[x, ]; View(d)})

# Go through each list of indices and apply the imputing function to each of them
res <- lapply(idx_extended, FUN = impute_missing_ToL, df = test_df) %>% reduce(rbind) %>% distinct(.keep_all = TRUE)
```


#### Approach 3:
For non-padded data, calculate the half an hour value, basically just every other value:
1. Pick two rows at a time, check if their sectors are the same: if yes, linearly impute the middle value and a value for the sector; if no, set the value and sector to NA
2. Finally pad the original data frame with the values created: if pad_value can't be worth of multiple columns (= can't pad ToL and sector at the same time), then what?
OR:
1. Create a data frame with the half an hour time stamps (e.g. by padding the actual one)
2. Edit the copy of the actual by filling the interpolated ToL values
3. In the end, join/merge the two together such that the missing 30 min values from the actual one get replaced by the filled one


```{r}
#tol_padded <- tol %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)
time_seq <- seq(min(tol$Time) + minutes(30), max(tol$Time) - minutes(30), by = "1 hour")
```
```{r}
tol_padded <- tol %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)
#View(filter(tol_padded, is.na(ToL)))

ggplot(filter(tol, is.na(ToL)), aes(x = Time)) + geom_histogram() + ggtitle(paste(sum(nrow(filter(tol, is.na(ToL))))))
ggplot(filter(tol_padded, is.na(ToL)), aes(x = Time)) + geom_histogram() + ggtitle(paste(nrow(filter(tol_padded, is.na(ToL)))))

idx <- which(is.na(tol_padded$ToL))
idx_extended <- idx %>% lapply(surrounding_idx, df = tol_padded) %>% .[length(.) != 3]
#idx_extended <- lapply(idx, surrounding_idx)

res <- lapply(idx_extended, FUN = impute_missing_ToL, df = tol_padded) %>% reduce(rbind) %>% distinct(.keep_all = TRUE)

```
#### Approach 4:
1. Pad missing 30 min values
2. Choose rows for which the linear imputation works (works = same sector, non-NA values) to obtain a sub df
3. Impute missing values to sub df
4. Merge with og padded (or non-padded?) df

```{r}
tol_na_rem <- tol %>% drop_na()

# Grouping data by DayMonth leads to multiple consecutive sequences of row indices because the complete data has all years
t <- tol_na_rem %>% group_by(sector, DayMonth) %>% group_data()
sectors <- split(t, t$sector)
s1 <- sectors[[1]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
s2 <- sectors[[2]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
s3 <- sectors[[3]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
s4 <- sectors[[4]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))

pad_test <- function(x, df) {
  df <- pad_by_time(df[x,], Time, .by = "30 mins", .pad_value = NA)
  return(df)
}

# This didn't take forever so next just impute the missing ToL and sector values in each of these
padded_s1 <- lapply(s1, FUN = pad_test, df = tol_na_rem) %>% reduce(rbind) 
padded_s2 <- lapply(s2, FUN = pad_test, df = tol_na_rem) %>% reduce(rbind)
padded_s3 <- lapply(s3, FUN = pad_test, df = tol_na_rem) %>% reduce(rbind)
padded_s4 <- lapply(s4, FUN = pad_test, df = tol_na_rem) %>% reduce(rbind)

imputed_s1 <- lapply(padded_s1[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X))) # $ToL and $sector
imputed_s2 <- lapply(padded_s2[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
imputed_s3 <- lapply(padded_s3[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
imputed_s4 <- lapply(padded_s4[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))

padded_s1[2:3] = imputed_s1
padded_s2[2:3] = imputed_s2
padded_s3[2:3] = imputed_s3
padded_s4[2:3] = imputed_s4

tol_imputed <- rbind(padded_s1, padded_s2, padded_s3, padded_s4)
tol_imputed <- tol_imputed %>% arrange(Time) %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)

```

```{r}

impute_missing_tol_data <- function(df) {
  tol_na_rem <- df %>% drop_na()
  
  # Grouping data by DayMonth leads to multiple consecutive sequences of row indices because the complete data has all years
  sectors <- tol_na_rem %>% group_by(sector) %>% group_data() %>% split(.$sector)
  
  #t <- tol_na_rem %>% group_by(sector, DayMonth) %>% group_data()
  #sectors <- split(t$nrows, t$sector)
  
  s1 <- sectors[[1]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s2 <- sectors[[2]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s3 <- sectors[[3]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  s4 <- sectors[[4]]$.rows %>% unlist %>% sort %>% split(cumsum(c(1, diff(.) != 1)))
  
  pad_30min <- function(x, df) {
    df <- pad_by_time(df[x,], Time, .by = "30 mins", .pad_value = NA)
    return(df)
  }
  
  # Pad each of the sections separately
  padded_s1 <- lapply(s1, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind) 
  padded_s2 <- lapply(s2, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  padded_s3 <- lapply(s3, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  padded_s4 <- lapply(s4, FUN = pad_30min, df = tol_na_rem) %>% reduce(rbind)
  
  imputed_s1 <- lapply(padded_s1[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s2 <- lapply(padded_s2[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s3 <- lapply(padded_s3[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  imputed_s4 <- lapply(padded_s4[2:3], function(X) approxfun(seq_along(X), X)(seq_along(X)))
  
  padded_s1[2:3] = imputed_s1
  padded_s2[2:3] = imputed_s2
  padded_s3[2:3] = imputed_s3
  padded_s4[2:3] = imputed_s4
  
  tol_imputed <- rbind(padded_s1, padded_s2, padded_s3, padded_s4)
  tol_imputed <- tol_imputed %>% arrange(Time) %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)
  
  return(tol_imputed)
}

```


```{r, warning=FALSE}

tol_imputed <- impute_missing_tol_data(tol)

```




```{r}
mydf <- data.frame(date = as.Date(c("2015-10-05","2015-10-08","2015-10-09",
                                    "2015-10-12","2015-10-14")),       
                   value = c(8,3,9,NA,5))

mydf
data.frame(date = seq(mydf$date[1], mydf$date[nrow(mydf)], by = 1)) %>% full_join(mydf, by = "date") %>% mutate(approx = na.approx(value))



t1 <- tol_na_rem %>% group_by(sector) %>% group_data() %>% split(.$sector)
View(t1[[1]])
t2 <- sectors[[1]]$.rows %>% unlist %>% sort
View(t2)
sum(unlist(t1[[1]]$.rows) != t2) # these are the same so grouping only by sector is enough

test_df <- tol_na_rem[1:10, ]
is <- list(list(1,2,3), list(7,8))
padded_test <- test_df[unlist(is), ] %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)
padded_test

sub_dfs <- lapply(is, function(x) {test_df[unlist(x), ] %>% pad_by_time(Time, .by = "30 mins", .pad_value = NA)}) #%>% lapply(x = 1:nrow(.), approxfun(method = "linear"))
sub_dfs[[1]]$ToL <- approxfun(sub_dfs[[1]]$ToL, method = "linear")(1:nrow(sub_dfs[[1]]))


sub_dfs[[1]]
```


---

Load and preprocess the SA measurement data
```{r}
preprocess_sa_data <- function(folder_path) {
  x <- list.files(path = folder_path, full.names = TRUE) %>%
    lapply(read.table) %>%
    lapply(function(x) {names(x) <- c("Time", "SA_cm3", "3", "4"); x}) %>%
    lapply(function(x) {x[3:4] <- list(NULL); x}) %>%
    lapply(function(x) {y <- mutate(x, Time=as.POSIXct((Time - 719529)*86400, origin = "1970-01-01", tz="UTC")); y}) %>%
    lapply(function(x) arrange(x, Time)) %>%
    reduce(rbind) %>%
    arrange(Time) %>%
    filter((SA_cm3 > -1e5) & (SA_cm3 < 1e9))
    return(x)
}

sa_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/sulphuric_acid/"
sa <- preprocess_sa_data(sa_path)

sa11 <- filter(sa, year(Time) == 2011)
sa12 <- filter(sa, year(Time) == 2012)
sa13 <- filter(sa, year(Time) == 2013)
sa18 <- filter(sa, year(Time) == 2018)
sa19 <- filter(sa, year(Time) == 2019)
sa22 <- filter(sa, year(Time) == 2022)

# SA data in 2011 and 2012 is measured every 2 minutes (other data is every 30 min). Before calculating the 30 min average, filter out possible outliers outliers using interquartile range (IQR): filter out data points that are further than 1.5 * IQR away from Q1 or Q3 of the data in the current window. If the window doesn't fit (in the beginning/end), just keep the data as is.

iqr_filter <- function(d, width, q1 = 0.25, q2 = 0.75) {
  filtered <- d %>%
    mutate(lower = rollapply(d$SA_cm3, width, FUN = function(x) { r <- quantile(x, probs = c(0.25, 0.75)); iqr <- r[2] - r[1]; lower_lim <- r[1] - 1.5*iqr; upper_lim <- r[2] + 1.5*iqr; lower_lim}, align = "center",  fill = NA)) %>%
    mutate(upper = rollapply(d$SA_cm3, width, FUN = function(x) { r <- quantile(x, probs = c(0.25, 0.75)); iqr <- r[2] - r[1]; lower_lim <- r[1] - 1.5*iqr; upper_lim <- r[2] + 1.5*iqr; upper_lim}, align = "center",  fill = NA)) %>%
    filter(is.na(lower) | between(SA_cm3, lower, upper)) %>%
    mutate(lower = NULL, upper = NULL)
  return(filtered)
}

sa11_filtered_30 <- iqr_filter(sa11, width = 15)
sa12_filtered_30 <- iqr_filter(sa12, width = 15)

sa11_filtered_60 <- iqr_filter(sa11, width = 31)
sa12_filtered_60 <- iqr_filter(sa12, width = 31)

sa11_filtered_240 <- iqr_filter(sa11, width = 121)
sa12_filtered_240 <- iqr_filter(sa12, width = 121)


# Round the SA values to 30 min
compute_SA_30min_average <- function(x) {
  df <- data.frame(x)
  df$Time <- floor_date(df$Time, "30 mins")
  df <- group_by(df, Time)
  df <- summarise(df, SA_cm3 = mean(SA_cm3, na.rm = FALSE))
  return(df)
}

sa11_rounded <- compute_SA_30min_average(sa11)
sa12_rounded <- compute_SA_30min_average(sa12)

sa11_rounded_f30 <- compute_SA_30min_average(sa11_filtered_30)
sa12_rounded_f30 <- compute_SA_30min_average(sa12_filtered_30)

sa11_rounded_f60 <- compute_SA_30min_average(sa11_filtered_60)
sa12_rounded_f60 <- compute_SA_30min_average(sa12_filtered_60)

sa11_rounded_f240 <- compute_SA_30min_average(sa11_filtered_240)
sa12_rounded_f240 <- compute_SA_30min_average(sa12_filtered_240)


# 2013 seems to contain duplicate rows. Keep only unique rows and round the dates to nearest 30 min to round times such as 11:29:59
sa13 <- sa13 %>% distinct(.keep_all = TRUE)
sa13$Time <- round_date(sa13$Time, unit = "30 mins")


# Round dates in 2018, 2019 and 2022 to the nearest 30 mins to round times such as 11:29:59
sa18$Time <- round_date(sa18$Time, unit = "30 mins")
sa19$Time <- round_date(sa19$Time, unit = "30 mins")
sa22$Time <- round_date(sa22$Time, unit = "30 mins")

sa_data <- reduce(list(sa11_rounded, sa12_rounded, sa13, sa18, sa19, sa22), rbind)
sa_data_30 <- reduce(list(sa11_rounded_f30, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
sa_data_60 <- reduce(list(sa11_rounded_f60, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
sa_data_240 <- reduce(list(sa11_rounded_f240, sa12_rounded_f30, sa13, sa18, sa19, sa22), rbind)
```


Load and preprocess the condensation sink data
```{r}
cs_path <- "data/hyytiala/condensation_sink/CS_2004_2023_10min.txt"
cs_all <- read.table(cs_path, col.names = c("Year", "Month", "Day", "Hour", "Minute", "Second", "CS_rate"))
cs <- filter(cs_all, (Year >= 2011 & Year <= 2013) | (Year >= 2018 & Year <= 2019) | (Year == 2022))
cs$Time <- as.POSIXct(str_c(cs$Year, "-", cs$Month, "-", cs$Day, " ", cs$Hour, ":", cs$Minute, ":00"), format="%Y-%m-%d %H:%M:%S", tz="UTC")
cs <- cs[!names(cs) %in% c("Minute","Second", "Hour", "Day", "Year", "Month")]

cs$Time <- floor_date(cs$Time, "30 mins")
cs <- group_by(cs, Time)
cs <- summarise(cs, CS_rate = mean(CS_rate, na.rm = FALSE))
```


Combine SA, CS and the SMEAR data into one data frame and write to file.
```{r}

#all_data_list <- list()

all_data <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data, cs, tol_imputed))
all_data1 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_30, cs, tol_imputed))
all_data2 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_60, cs, tol_imputed))
all_data3 <- Reduce(function(x, y) merge(x, y, all=TRUE), list(smear_data, sa_data_240, cs, tol_imputed))

#tmp <- merge(smear_data, sa_data, by = "Time", all = TRUE)
#all_data <- merge(tmp, cs, by = "Time", all = TRUE)
#tmp <- merge(tmp, cs, by = "Time", all = TRUE)
#all_data <- merge(tmp, tol_imputed, by = "Time", all = TRUE)

#tmp <- merge(smear_data, sa_data_30, by = "Time", all = TRUE)
#all_data_1 <- merge(tmp, cs, by = "Time", all = TRUE)

#tmp <- merge(smear_data, sa_data_60, by = "Time", all = TRUE)
#all_data_2 <- merge(tmp, cs, by = "Time", all = TRUE)

#tmp <- merge(smear_data, sa_data_240, by = "Time", all = TRUE)
#all_data_3 <- merge(tmp, cs, by = "Time", all = TRUE)

write.csv(all_data, "data/all_data_merged.csv", row.names = FALSE)
write.csv(all_data1, "data/all_data_merged_f30.csv", row.names = FALSE)
write.csv(all_data2, "data/all_data_merged_f60.csv", row.names = FALSE)
write.csv(all_data3, "data/all_data_merged_f240.csv", row.names = FALSE)
```


# Plot SMEAR data

```{r}
smear_data <- reduce(list(hyytiala1113, hyytiala1819, hyytiala22), rbind)
gr <- filter(smear_data, global_radiation < 0)
nox <- filter(smear_data, NOx < 0)
so2 <- filter(smear_data, SO2 < 0)

l <- list(
  p1 <- ggplot(gr, aes(x=hour(Time))) + geom_histogram(na.rm=TRUE, binwidth = 1) + ggtitle("Global radiation"),
  p2 <- ggplot(nox, aes(x=hour(Time))) + geom_histogram(na.rm=TRUE, binwidth = 1) + ggtitle("NOx"),
  p3 <- ggplot(so2, aes(x=hour(Time))) + geom_histogram(na.rm=TRUE, binwidth = 1) + ggtitle("SO2")
)

p1 <- ggarrange(plotlist = l, common.legend = TRUE, nrow = 1, ncol = 3, legend = "bottom")
p1

```


# Plot filtered SA data

Load and distribute data
```{r}
sa_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/sulphuric_acid/"
sa <- preprocess_sa_data(sa_path)

sa11 <- filter(sa, year(Time) == 2011)
sa12 <- filter(sa, year(Time) == 2012)
sa13 <- filter(sa, year(Time) == 2013)
sa18 <- filter(sa, year(Time) == 2018)
sa19 <- filter(sa, year(Time) == 2019)
sa22 <- filter(sa, year(Time) == 2022)
```

Define helper functions
```{r}

iqr_flag_outliers <- function(d, width, q1 = 0.25, q2 = 0.75) {
    flagged <- d %>%
    mutate(lower = rollapply(d$SA_cm3, width, FUN = function(x) { r <- quantile(x, probs = c(0.25, 0.75)); iqr <- r[2] - r[1]; lower_lim <- r[1] - 1.5*iqr; upper_lim <- r[2] + 1.5*iqr; lower_lim}, align = "center",  fill = NA)) %>%
    mutate(upper = rollapply(d$SA_cm3, width, FUN = function(x) { r <- quantile(x, probs = c(0.25, 0.75)); iqr <- r[2] - r[1]; lower_lim <- r[1] - 1.5*iqr; upper_lim <- r[2] + 1.5*iqr; upper_lim}, align = "center",  fill = NA)) %>%
    mutate(outlier = !(is.na(lower) | between(SA_cm3, lower, upper))) %>%
    mutate(lower = NULL, upper = NULL)
  return(flagged)
}

iqr_plot_flagged_outliers <- function(df, s = "") {
  ggplot(df, aes(x = Time, y = SA_cm3, color = outlier)) +
  geom_point(size = 2, na.rm = TRUE) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_minimal() +
  labs(title = paste("SA cm-3 ", s),
       x = "Date",
       y = "SA_3cm",
       color = "Outlier")
}

get_comparison_df <- function(original_df) {
  
  # unfiltered average
  unfiltered <- compute_SA_30min_average(original_df)
  
  # filter data
  filtered_30 <- iqr_filter(original_df, width = 15)
  filtered_60 <- iqr_filter(original_df, width = 31)
  filtered_240 <- iqr_filter(original_df, width = 121)
  

  rounded1 <- compute_SA_30min_average(filtered_30)
  rounded2 <- compute_SA_30min_average(filtered_60)
  rounded3 <- compute_SA_30min_average(filtered_240)
  
  colnames(rounded1)[2] <- "SA_cm3_30"
  colnames(rounded2)[2] <- "SA_cm3_60"
  colnames(rounded3)[2] <- "SA_cm3_240"

  comp <- reduce(list(unfiltered, rounded1, rounded2, rounded3), cbind)
  comp <- comp[unique(colnames(comp))]

  #comp <- comp %>% mutate(diff1 = SA_cm3 - SA_cm3_30) %>% mutate(diff2 = SA_cm3 - SA_cm3_60) %>% mutate(diff3 = SA_cm3 - SA_cm3_240)

  comp_long <- comp %>%
  pivot_longer(cols = starts_with("SA_cm3"), names_to = "variable", values_to = "value")
  
  return(comp_long)
}

plot_comparison <- function(df, time) {
  #ggplot(df, aes(x = Time, y = value, color = variable)) +
  #geom_point() +
  #theme_minimal() +
  #labs(title = paste("Differences between unfiltered and filtered SA values, ", time),
  #     x = "Time",
  #     y = "value",
  #     color = "variable")
  
  ggplot(df, aes(x = Time, y = value, color = variable)) +
  geom_line(data = subset(df, variable == "SA_cm3"), aes(x = Time, y = value) , color = "black") +
  geom_point(data = subset(df, variable == "SA_cm3_30"), aes(x = Time, y = value), color = "orange") +
  geom_point(data = subset(df, variable == "SA_cm3_60"), aes(x = Time, y = value), color = "red") +
  geom_point(data = subset(df, variable == "SA_cm3_240"), aes(x = Time, y = value), color = "blue") +
  theme_minimal() +
  labs(title = paste("Unfiltered and filtered SA values", time),
       x = "Time",
       y = "SA_cm3",
       color = "variable")
}
```

Compare the filtering results with different window width
```{r}
sa11_comp <- get_comparison_df(sa11)
sa12_comp <- get_comparison_df(sa12)

sa12_og_rounded <- compute_SA_30min_average(sa12)
s <- filter(sa12_og_rounded, month(Time) == 3 & day(Time) == 27)
ggplot(s, aes(Time, SA_cm3)) + geom_line(na.rm=TRUE) + ggtitle("2012") + scale_y_continuous(trans='log10')

sa11_comp_filtered <- filter(sa11_comp, day(Time) == 23)
sa12_comp_filtered <- filter(sa12_comp, month(Time) == 3 & day(Time) == 27)

plot_comparison(sa11_comp_filtered, 2011)
plot_comparison(sa12_comp_filtered, 2012)

# Create the plot
ggplot(sa12_comp, aes(x = Time, y = value, color = variable)) +
  geom_line() +
  theme_minimal()

f13 <- iqr_flag_outliers(sa18, 31)
f22 <- filter(f13, month(Time) == 7 & day(Time) == 18)
iqr_plot_flagged_outliers(f22, "2018")

h18 <- filter(hyytiala1819, year(Time) == 2018 & month(Time) == 7 & day(Time) >= 18 & day(Time) <= 19)
ggplot(h18, aes(Time, HYY_META.SO2168)) + geom_point(na.rm=TRUE) + ggtitle("2013") #+ scale_y_continuous(trans='log10')
```

Plot the whole 2011 and 2012 and some random days to see which values would get filtered out
```{r}

ggplot(sa11_filtered_60, aes(x = Time, y = SA_cm3)) +
  geom_point(size = 2) +
  theme_minimal() +
  labs(title = "SA cm-3, 2011",
       x = "Date",
       y = "SA_3cm") #+
  #scale_y_continuous(trans='log10')

sa11_flagged <- iqr_flag_outliers(sa11, width = 31)
sa12_flagged <- iqr_flag_outliers(sa12, width = 31)

sa12_flagged_sample <- filter(sa12_flagged, month(Time) == 4 & day(Time) == 12 & hour(Time) >= 16 & hour(Time) <= 18)
ggplot(sa12_flagged_sample, aes(x = Time, y = SA_cm3, color = outlier)) +
  geom_point(size = 2, na.rm = TRUE) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_minimal() +
  labs(title = "SA cm-3, 2011",
       x = "Date",
       y = "SA_3cm",
       color = "Outlier") #+
  #scale_y_continuous(trans='log10')

```


# Plot unprocessed SA data

Load SA data, add a Year column in SA dataframes and combine them into one
```{r}
sa_path <- "/scratch/dongelr1/susannar/kesa2024/data/hyytiala/sulphuric_acid/"
sa <- preprocess_sa_data(sa_path)

sa <- sa %>% mutate(Year = paste(year(Time)))
sa$DayMonth <- format(sa$Time, "%d/%m")


sa11 <- filter(sa, year(Time) == 2011)
sa12 <- filter(sa, year(Time) == 2012)
sa13 <- filter(sa, year(Time) == 2013)
sa18 <- filter(sa, year(Time) == 2018)
sa19 <- filter(sa, year(Time) == 2019)
sa22 <- filter(sa, year(Time) == 2022)

```

Plot SA data by year
```{r}

plot_list <- list(
  p2011 <- ggplot(sa11, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2011"),
  p2012 <- ggplot(sa12, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2012"),
  p2013 <- ggplot(sa13, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2013"),
  p2018 <- ggplot(sa18, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2018"),
  p2019 <- ggplot(sa19, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2019"),
  p2022 <- ggplot(sa22, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2022")
)

gridExtra::grid.arrange(grobs = plot_list)
```
Plot all years in log scale
```{r}

plot_list <- list(
  p11 <- ggplot(sa11, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2011") + scale_y_continuous(trans='log10') + ylab("log(SA)"),
  p12 <- ggplot(sa12, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2012") + scale_y_continuous(trans='log10') + ylab("log(SA)"),
  p13 <- ggplot(sa13, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2013") + scale_y_continuous(trans='log10') + ylab("log(SA)"),
  p18 <- ggplot(sa18, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2018") + scale_y_continuous(trans='log10') + ylab("log(SA)"),
  p19 <- ggplot(sa19, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2019") + scale_y_continuous(trans='log10') + ylab("log(SA)"),
  p22 <- ggplot(sa22, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2022") + scale_y_continuous(trans='log10') + ylab("log(SA)")
)

#gridExtra::grid.arrange(grobs = plot_list)
p1 <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 2, ncol = 3, legend = "bottom")
p1
```


Boxplot for SA in log scale for each year
```{r}

bplot_list = list(
  p2018 <- ggplot(sa18, aes(Time, SA_cm3)) + geom_boxplot(outlier.size=4, na.rm=TRUE) + ggtitle("2018") + scale_y_continuous(trans='log10'),
  p2019 <- ggplot(sa19, aes(Time, SA_cm3)) + geom_boxplot(outlier.size=4, na.rm=TRUE) + ggtitle("2019") + scale_y_continuous(trans='log10'),
  p2022 <- ggplot(sa22, aes(Time, SA_cm3)) + geom_boxplot(outlier.size=4, na.rm=TRUE) + ggtitle("2022") + scale_y_continuous(trans='log10'),
  p2011 <- ggplot(sa11, aes(Time, SA_cm3)) + geom_boxplot(outlier.size=4, na.rm=TRUE) + ggtitle("2011") + scale_y_continuous(trans='log10'),
  p2012 <- ggplot(sa12, aes(Time, SA_cm3)) + geom_boxplot(outlier.size=4, na.rm=TRUE) + ggtitle("2012") + scale_y_continuous(trans='log10'),
  p2013 <- ggplot(sa13, aes(Time, SA_cm3)) + geom_boxplot(outlier.size=4, na.rm=TRUE) + ggtitle("2013") + scale_y_continuous(trans='log10')
)

p <- ggarrange(plotlist = bplot_list, common.legend = TRUE, nrow = 2, ncol = 3, legend = "bottom")
p
```

Plot all years within the same boxplot (execpt 2013)

```{r}

bplots1 <- ggplot(sa_all[sa$Year %in% c("2011", "2012", "2018", "2019", "2022"),], aes(x=Year, y=SA_cm3, group = Year)) + geom_boxplot(na.rm=TRUE)
bplots2013 <- ggplot(sa_all[sa$Year %in% c("2013"),], aes(x=Year, y=SA_cm3)) + geom_boxplot(na.rm=TRUE)

bplots1
bplots2013


```


Plot all years as a line/dot plot with the same x axis

```{r}

month_breaks <- as.Date(paste0("2000-", 1:12, "-01"))
month_labels <- month.abb
sa$Date <- as.Date(paste0("2000/", sa$DayMonth), format="%Y/%d/%m")

p <- ggplot(sa, aes(x = Date, y = SA_cm3)) +
  geom_point(na.rm = TRUE) +
  facet_wrap(~ Year, scales = "free_y", ncol = 2, nrow = 3)
p

```

## Plot some random days

Plot only 2013 June when there seems to be more extreme values, compare to the SO2 data from the same day
```{r}
sa13_june <- filter(sa13, month(Time) == 6 & day(Time) >= 27)
p1 <- ggplot(sa13_june, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("SA, June 2013")

sa13_june <- filter(sa13_june, day(Time) >= 27)
p2 <- ggplot(sa13_june, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("SA, June 2013") + scale_y_continuous(trans='log10') + ylab("log(SA)") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

smear11 <- filter(hyytiala1113, year(Time) == 2011 & month(Time) == 6 & day(Time) >= 27)
p3 <- ggplot(smear11, aes(Time, HYY_META.SO2168)) + geom_point(na.rm=TRUE) + ggtitle("SO2, June 2013") + scale_y_continuous(trans='log10') + ylab("log(SO2)") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_list = list(p2, p3)
#gridExtra::grid.arrange(grobs = plot_list, nrow = 1, ncol = 3)
p2 <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 1, ncol = 2, legend = "bottom")
p2

```

Similarly with 2019
```{r}
sa19_jan <- filter(sa19, month(Time) == 1 & day(Time) >= 24 & day(Time) <= 25)
p1 <- ggplot(sa19_jan, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("SA, 2019") + scale_y_continuous(trans='log10') + ylab("log(SA)") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

smear19 <- filter(hyytiala1819, year(Time) == 2019 & month(Time) == 1 & day(Time) >= 24 & day(Time) <= 25)
p2 <- ggplot(smear19, aes(Time, HYY_META.SO2168)) + geom_point(na.rm=TRUE) + ggtitle("SO2, 2019") + scale_y_continuous(trans='log10') + ylab("log(SO2)") + theme(axis.text.x = element_text(angle = 45, hjust = 1))


plot_list = list(p1, p2)
p <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 1, ncol = 2, legend = "bottom")
p
```

CLoser look at April 2011, SO2 not available
```{r}
#sa11_spring <- filter(sa11, month(Time) >= 4 & month(Time) <= 4)
#p1 <- ggplot(sa11_spring, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("SA, 2011") + scale_y_continuous(trans='log10') + ylab("log(SA)")
p1 <- ggplot(sa11, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2011") + scale_y_continuous(trans='log10') + ylab("log(SA)")

#smear11 <- filter(hyytiala1113, Time %in% sa11$Time)
smear11 <- filter(hyytiala1113, year(Time) == 2011 & month(Time) >= 3 & month(Time) <= 4)
p2 <- ggplot(smear11, aes(Time, HYY_META.SO2168)) + geom_point(na.rm=TRUE) + ggtitle("SO2, June 2011") + scale_y_continuous(trans='log10') + ylab("log(SO2)") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

plot_list = list(p1, p2)
p <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 1, ncol = 2, legend = "bottom")
p
```

Plot only negative data for each year
```{r}

nsa11 <- filter(sa11, SA_cm3 < 0)
nsa12 <- filter(sa12, SA_cm3 < 0)
nsa13 <- filter(sa13, SA_cm3 < 0)
nsa18 <- filter(sa18, SA_cm3 < 0)
nsa19 <- filter(sa19, SA_cm3 < 0)
nsa22 <- filter(sa22, SA_cm3 < 0)

plot_list <- list(
  p11 <- ggplot(nsa11, aes(x=Time)) + geom_histogram(na.rm=TRUE) + ggtitle("2011") + theme(axis.text.x = element_text(angle = 45, hjust = 1)),
  p12 <- ggplot(nsa12, aes(x=Time)) + geom_histogram(na.rm=TRUE) + ggtitle("2012") + theme(axis.text.x = element_text(angle = 45, hjust = 1)),
  p13 <- ggplot(nsa13, aes(x=Time)) + geom_histogram(na.rm=TRUE) + ggtitle("2013") + theme(axis.text.x = element_text(angle = 45, hjust = 1))
  #p18 <- ggplot(nsa18, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2018"),
  #p19 <- ggplot(nsa19, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2019"),
  #p22 <- ggplot(nsa22, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2022")
)

#gridExtra::grid.arrange(grobs = plot_list)
p3 <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 1, ncol = 3, legend = "bottom")
p3 <- annotate_figure(p3, top = text_grob("Negative values yearly"))

p3


```


Histograms of negative values yearly
```{r}
p11_1 <- ggplot(nsa11, aes(x=hour(Time))) + geom_histogram(binwidth = 1) + ggtitle("Negative values by hour, 2011") + theme_minimal() 
p11_2 <- ggplot(nsa11, aes(x=SA_cm3)) + geom_histogram(bins=30) + ggtitle("Negative values, 2011") + theme_minimal() 

p11 <- ggarrange(plotlist = list(p11_1, p11_2), common.legend = TRUE, nrow = 1, ncol = 2, legend = "bottom")
p11


p12_1 <- ggplot(nsa12, aes(x=hour(Time))) + geom_histogram(binwidth = 1) + ggtitle("Negative values by hour, 2012") + theme_minimal() 
p12_2 <- ggplot(nsa12, aes(x=SA_cm3)) + geom_histogram(bins=30) + ggtitle("Negative values, 2012") + theme_minimal() 
p12 <- ggarrange(plotlist = list(p12_1, p12_2), common.legend = TRUE, nrow = 1, ncol = 2, legend = "bottom")
p12


plot_list = list(
  p11 <- ggplot(nsa11, aes(x=hour(Time))) + geom_histogram(binwidth = 1) + ggtitle("2011") + theme_minimal(),
  p12 <- ggplot(nsa12, aes(x=hour(Time))) + geom_histogram(binwidth = 1) + ggtitle("2012") + theme_minimal(),
  p13 <- ggplot(nsa13, aes(x=hour(Time))) + geom_histogram(binwidth = 1) + ggtitle("2013") + theme_minimal()
)

p1 <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 1, ncol = 3, legend = "bottom")

plot_list = list(
  p11 <- ggplot(nsa11, aes(x=SA_cm3)) + geom_histogram(bins=30) + ggtitle("2011") + theme_minimal(),
  p12 <- ggplot(nsa12, aes(x=SA_cm3)) + geom_histogram(bins=30) + ggtitle("2012") + theme_minimal(),
  p13 <- ggplot(nsa13, aes(x=SA_cm3)) + geom_histogram(bins=30) + ggtitle("2013") + theme_minimal()
)

p2 <- ggarrange(plotlist = plot_list, common.legend = TRUE, nrow = 1, ncol = 3, legend = "bottom")


annotate_figure(p1, top = text_grob("Negative values by hour"))#, 
               #color = "red", face = "bold", size = 14))

annotate_figure(p2, top = text_grob("Distribution of negative SA values"))#, 
               #color = "red", face = "bold", size = 14))

```

Plot some days from 2011
```{r}
p1 <- ggplot(sa11, aes(Time, SA_cm3)) + geom_line(na.rm=TRUE) + ggtitle("2011, as is")
p1

a4 <- filter(sa11, month(Time) == 4 & day(Time) == 2 & hour(Time) >= 7 & hour(Time) <= 8)
ggplot(a4, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2011, a4") #+ scale_y_continuous(trans='log10')


a1 <- filter(sa11, month(Time) == 4 & day(Time) == 1 & hour(Time) >= 6 & hour(Time) <= 12)
ggplot(a1, aes(Time, SA_cm3)) + geom_point(na.rm=TRUE) + ggtitle("2011, a1")# + scale_y_continuous(trans='log10')
```